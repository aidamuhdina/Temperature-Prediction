# -*- coding: utf-8 -*-
"""Salinan time series baru guys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1epP-a2HEjLbomw6T7ltcXpjfOJJCHYbL

Nama : Aida Muhdina
</br>Email : aidamuhdina@gmail.com
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

data = pd.read_csv('/content/drive/MyDrive/Colab machine learning/time series/city_temperature.csv')
data

data = data.loc[(data.City == 'Washington')]
data = data.loc[(data.Year >= 2000) & (data.Year <= 2020)]
data

#menggabungkan kolom "month", "day", "year" menjadi "date"
data['Date'] = pd.to_datetime(data.loc[:,('Year', 'Month', 'Day')], format='%Y%m%d')
data

#menghapus kolom yang tidak diperlukan
data.drop(['Region', 'State', 'City', 'Country','Year','Month', 'Day'], axis=1, inplace=True)
data

data.AvgTemperature.replace(-99, np.NaN, inplace=True)
data.AvgTemperature.ffill(inplace=True)
data.sort_values(by='AvgTemperature', ascending=True)

# #membuat plot dataset
date = data['Date'].values
average_temp = data['AvgTemperature'].values

plt.figure(figsize=(15, 5))
plt.plot(date, average_temp)
plt.title('Average Temperature', fontsize=20)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

train, val = train_test_split(average_temp, test_size=0.2, shuffle=False)

train_set = windowed_dataset(train, window_size=60, batch_size=100, shuffle_buffer=1000)
val_set = windowed_dataset(val, window_size=60, batch_size=100, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
            tf.keras.layers.LSTM(60, return_sequences=True),
            tf.keras.layers.LSTM(60),
            tf.keras.layers.Dense(10, activation='relu'),
            tf.keras.layers.Dense(1)
])

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-05, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=['mae'])

#membuat callback
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)

history = model.fit(train_set, epochs=100, validation_data=val_set, callbacks=[early_stop], verbose=2)

#plot mae
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model Mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()